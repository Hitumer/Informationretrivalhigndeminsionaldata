{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Recall the data processing routines from the last lab course. The following excercises build on top of the extracted feature representations, but instead of the prebuilt classifier, we want to implement logistic regression by hand. To this end, make sure, that the variables `train`, `test`, `train_data_features` and `test_data_features` are loaded to your IPython shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#%% download nltk stopwords\n",
    "# import nltk\n",
    "# ntlk.download('stopwords')\n",
    "\n",
    "# load stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "# function for preprocessing the data\n",
    "def review_prepro(data, remove_stopwords=False):\n",
    "    # remove HTML tags\n",
    "    review_text = BeautifulSoup(data, 'lxml').get_text()\n",
    "    # remove non-letters and numbers\n",
    "    letters_only = re.sub( '[^a-zA-Z]',\n",
    "                          ' ',\n",
    "                          review_text )\n",
    "    # make all characters lower case and split the documents into single words\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        # remove stop words\n",
    "        meaningful_words = [ w for w in words if not w in stops ]\n",
    "        # return concatenated single string\n",
    "        return ' '.join(meaningful_words)\n",
    "    else:\n",
    "        # or don't and concatenate to single string\n",
    "        return ' '.join(words)\n",
    "\n",
    "# load data as pandas dataframe\n",
    "train = pd.read_csv('labeledTrainData.tsv', \n",
    "                    header=0,\n",
    "                    delimiter=\"\\t\", \n",
    "                    quoting=3 )\n",
    "\n",
    "test = pd.read_csv('labeledTestData.tsv', \n",
    "                   header=0,\n",
    "                   delimiter=\"\\t\",\n",
    "                   quoting=3 )\n",
    "\n",
    "\n",
    "# preprocess train and test data\n",
    "num_reviews = train['review'].size\n",
    "\n",
    "clean_train_reviews = []\n",
    "for i in range(num_reviews):\n",
    "  #  if (i+1)%1000 == 0:\n",
    "    #    print('Review {} of {}\\n'.format(i+1, num_reviews))\n",
    "    clean_train_reviews.append( review_prepro(train['review'][i], remove_stopwords=True) )\n",
    "    \n",
    "num_test_reviews = test['review'].size\n",
    "\n",
    "clean_test_reviews = []\n",
    "for i in range(num_test_reviews):\n",
    "    #if (i+1)%1000 == 0:\n",
    "     #   print('Review {} of {}\\n'.format(i+1, num_test_reviews))\n",
    "    clean_test_reviews.append( review_prepro(test['review'][i], remove_stopwords=True) )\n",
    "    \n",
    "\n",
    "#%% create BoW\n",
    "# Documentation:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = stops,\n",
    "                             max_features = 5000)\n",
    "\n",
    "# fit the vectorizer and return transformed reviews\n",
    "vectorizer.fit(clean_train_reviews)\n",
    "train_data_features = vectorizer.transform(clean_train_reviews)\n",
    "clean_train_reviews=None \n",
    "# convert to numpy array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "# create BoW representation of test data\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "clean_test_reviews=None\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 5000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5399929762484854e-05"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Write a PYTHON function `logistic_gradient` that expects a training set matrix `X_train`, a ground truth label vector `y_train` and a current weight vector `w` as its input and returns the gradient `g` of the negative log-likelihood function of the logistic regression. Refer to the lecture notes for the mathematical definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    z = np.exp(-np.abs(x))\n",
    "    return np.where(x>=0.0,1.0/(1.0+z),z/(1.0+z))\n",
    "\n",
    "def logistic_gradient(X_train,y_train,w,reg=0.0):\n",
    "    # Rows are variables, Columns are samples\n",
    "    g=np.dot(X_train*np.expand_dims(y_train,0),-sigmoid(-np.dot(X_train.T, w)*y_train))/X_train.shape[1]+2*reg*w\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Write a PYTHON function `find_w` that expects a training set matrix `X_train`, a ground truth label vector `y_train`, a step size `alpha` and a maximum iteration number `max_it` that determines the optimal logistic regression weight vector `w_star` by performing gradient descent, i.e. calling `logistic_gradient` in each iteration. Make sure to include the affine offset $w_0=b$ into your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_w(X_train, y_train, alpha, n_epochs, n_minibatch, reg=0.0,w_init=None):\n",
    "    \n",
    "    X_offs=np.ones((X_train.shape[0]+1,X_train.shape[1]))\n",
    "    X_offs[1:,:]=X_train\n",
    "    w=np.ones((X_offs.shape[0]))\n",
    "    if w_init is not None:\n",
    "        w=w_init\n",
    "    loss=np.inf\n",
    "    for epoch in range(n_epochs):\n",
    "        zzz=-np.log(sigmoid(np.expand_dims(y_train,0)*np.dot(w.T,X_offs)))\n",
    "        loss=np.sum(zzz)/X_offs.shape[1]+reg*np.sum(w**2)\n",
    "        print('Epoch:', epoch+1, ' Current loss:', loss)\n",
    "        rp=np.random.permutation(X_offs.shape[1])\n",
    "        for it in range(X_offs.shape[1]//n_minibatch):\n",
    "            delta_w=logistic_gradient(X_offs[:,rp[it*n_minibatch:(it+1)*n_minibatch]],\n",
    "                                      y_train[rp[it*n_minibatch:(it+1)*n_minibatch]],w,reg)*alpha\n",
    "            w-=np.array(delta_w)\n",
    "        w_star=w\n",
    "    return w_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) The dataset at hand is quite large. Applying standard gradient descent could likely lead to Python throwing a `MemoryError` exception. To avoid this, we will employ a variation of *stochastic gradient descent* that has been proven successful in training deep neural networks. In *minibatch learning*, each iteration of the algorithm is replaced by a so-called *epoch*. In each epoch the training set is divided randomly into subsets of equal size, the minibatches. For each minibatch, the gradient is computed and applied only with respect to the samples in the minibatch. An epoch is finished when the gradient step was performed for each minibatch. Modify `find_w` such that it is capable of mini-batch learning. You will need to replace `max_it` by `n_epochs` and and add the parameter `n_minibatch` to your function definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "d) Write a function `classify_log` that expects a weight vector `w` and a test set matrix `X_test` and classifies the samples in `X_test` via logistic regression, returning a label vector `y_test`. Test your implementation of `find_w` and`classify_log` on `train_data_features` and `test_data_features` with 10 epochs, minibatches of size 100 and step size `alpha=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Current loss: 48.15060140614601\n",
      "Epoch: 2  Current loss: 1.6172408489893622\n",
      "Epoch: 3  Current loss: 0.9433528880277458\n",
      "Epoch: 4  Current loss: 0.6799919706002927\n",
      "Epoch: 5  Current loss: 0.5268252814330623\n",
      "Epoch: 6  Current loss: 0.4416676666389833\n",
      "Epoch: 7  Current loss: 0.373741098487843\n",
      "Epoch: 8  Current loss: 0.3276743398749519\n",
      "Epoch: 9  Current loss: 0.298225101222139\n",
      "Epoch: 10  Current loss: 0.2755530354401123\n",
      "Epoch: 11  Current loss: 0.2570490725664857\n",
      "Epoch: 12  Current loss: 0.23027984323976725\n",
      "Epoch: 13  Current loss: 0.2504629391698098\n",
      "Epoch: 14  Current loss: 0.2152871014820923\n",
      "Epoch: 15  Current loss: 0.19738760143619938\n",
      "Epoch: 16  Current loss: 0.18878666957583998\n",
      "Epoch: 17  Current loss: 0.18479011744847215\n",
      "Epoch: 18  Current loss: 0.17398627369707861\n",
      "Epoch: 19  Current loss: 0.16860476963757315\n",
      "Epoch: 20  Current loss: 0.16786245980493567\n",
      "AUC score after 20 epochs: 0.921913595668573\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xb/54n0yjj110577z7wbb09w97c0000gn/T/ipykernel_48377/3445832402.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'AUC score after 20 epochs:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mauc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "\n",
    "def classify_log(w, X_test):\n",
    "    w0=w[0]\n",
    "    y_test=sigmoid(w0+np.dot(X_test.T,w[1:]))\n",
    "    return y_test\n",
    "\n",
    "#Testing implementation\n",
    "y_train=train['sentiment'].values\n",
    "y_test=test['sentiment'].values\n",
    "w=find_w(train_data_features.T,(y_train-0.5)*2,1,20,100)\n",
    "y_pred=classify_log(w,test_data_features.T)\n",
    "auc = AUC( y_test, y_pred )\n",
    "print('AUC score after 20 epochs:',auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Logistic regression is prone to *overfitting*. To prevent this, *regularizers* can be used. Adjust your implementation in such a way that instead of minimizing $F(\\mathbf{w})$, it minimizes the term\n",
    "    \t\\begin{equation}\n",
    "    \tF(\\mathbf{w})+\\lambda\\|\\mathbf{w}\\|^2,\n",
    "    \t\\end{equation}\n",
    "    \twhere $\\lambda$ is a non-negative regularization parameter. Test your implementation with $\\lambda=10^{-3}$.  Did the results improve? Why/why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Current loss: 53.15160140614601\n",
      "Epoch: 2  Current loss: 2.3983379266146203\n",
      "Epoch: 3  Current loss: 0.9712014727220952\n",
      "Epoch: 4  Current loss: 0.5479802835562944\n",
      "Epoch: 5  Current loss: 0.4115661431503295\n",
      "Epoch: 6  Current loss: 0.3336861204811173\n",
      "Epoch: 7  Current loss: 0.3280795200479121\n",
      "Epoch: 8  Current loss: 0.41328751094507765\n",
      "Epoch: 9  Current loss: 0.4337439513355794\n",
      "Epoch: 10  Current loss: 0.311861515664448\n",
      "Epoch: 11  Current loss: 0.3260963451055826\n",
      "Epoch: 12  Current loss: 0.3262866656495476\n",
      "Epoch: 13  Current loss: 0.32219178471645615\n",
      "Epoch: 14  Current loss: 0.3105072211499323\n",
      "Epoch: 15  Current loss: 0.34888100885406814\n",
      "Epoch: 16  Current loss: 0.3142110512116592\n",
      "Epoch: 17  Current loss: 0.3159818062862258\n",
      "Epoch: 18  Current loss: 0.33113608713058\n",
      "Epoch: 19  Current loss: 0.372385040023918\n",
      "Epoch: 20  Current loss: 0.41242269174292584\n",
      "AUC score after 20 epochs: 0.9369066516511901\n"
     ]
    }
   ],
   "source": [
    "# def find_w(X_train, y_train, alpha, n_epochs, n_minibatch, reg=0.0,w_init=None):\n",
    "\n",
    "w=find_w(train_data_features.T,(y_train-0.5)*2,1,20,100,1e-3)\n",
    "y_pred=classify_log(w,test_data_features.T)\n",
    "auc = AUC( y_test, y_pred )\n",
    "print('AUC score after 20 epochs:',auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
