{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "Recall the data processing routines from the last lab course. The following excercises build on top of the extracted feature representations, but instead of the prebuilt classifier, we want to implement logistic regression by hand. To this end, make sure, that the variables `train`, `test`, `train_data_features` and `test_data_features` are loaded to your IPython shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/zyairexie/nltk_data'\n    - '/Users/zyairexie/opt/anaconda3/nltk_data'\n    - '/Users/zyairexie/opt/anaconda3/share/nltk_data'\n    - '/Users/zyairexie/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - '/Users/zyairexie/nltk_data'\n    - '/Users/zyairexie/opt/anaconda3/nltk_data'\n    - '/Users/zyairexie/opt/anaconda3/share/nltk_data'\n    - '/Users/zyairexie/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xb/54n0yjj110577z7wbb09w97c0000gn/T/ipykernel_37960/1824236960.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# load stopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mstops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# function for preprocessing the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{zip_name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{self.subdir}/{self.__name}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - '/Users/zyairexie/nltk_data'\n    - '/Users/zyairexie/opt/anaconda3/nltk_data'\n    - '/Users/zyairexie/opt/anaconda3/share/nltk_data'\n    - '/Users/zyairexie/opt/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#%% download nltk stopwords\n",
    "# import nltk\n",
    "# ntlk.download('stopwords')\n",
    "\n",
    "# load stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "# function for preprocessing the data\n",
    "def review_prepro(data, remove_stopwords=False):\n",
    "    # remove HTML tags\n",
    "    review_text = BeautifulSoup(data, 'lxml').get_text()\n",
    "    # remove non-letters and numbers\n",
    "    letters_only = re.sub( '[^a-zA-Z]',\n",
    "                          ' ',\n",
    "                          review_text )\n",
    "    # make all characters lower case and split the documents into single words\n",
    "    words = letters_only.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        # remove stop words\n",
    "        meaningful_words = [ w for w in words if not w in stops ]\n",
    "        # return concatenated single string\n",
    "        return ' '.join(meaningful_words)\n",
    "    else:\n",
    "        # or don't and concatenate to single string\n",
    "        return ' '.join(words)\n",
    "\n",
    "# load data as pandas dataframe\n",
    "train = pd.read_csv('labeledTrainData.tsv', \n",
    "                    header=0,\n",
    "                    delimiter=\"\\t\", \n",
    "                    quoting=3 )\n",
    "\n",
    "test = pd.read_csv('labeledTestData.tsv', \n",
    "                   header=0,\n",
    "                   delimiter=\"\\t\",\n",
    "                   quoting=3 )\n",
    "\n",
    "\n",
    "# preprocess train and test data\n",
    "num_reviews = train['review'].size\n",
    "\n",
    "clean_train_reviews = []\n",
    "for i in range(num_reviews):\n",
    "  #  if (i+1)%1000 == 0:\n",
    "    #    print('Review {} of {}\\n'.format(i+1, num_reviews))\n",
    "    clean_train_reviews.append( review_prepro(train['review'][i], remove_stopwords=True) )\n",
    "    \n",
    "num_test_reviews = test['review'].size\n",
    "\n",
    "clean_test_reviews = []\n",
    "for i in range(num_test_reviews):\n",
    "    #if (i+1)%1000 == 0:\n",
    "     #   print('Review {} of {}\\n'.format(i+1, num_test_reviews))\n",
    "    clean_test_reviews.append( review_prepro(test['review'][i], remove_stopwords=True) )\n",
    "    \n",
    "\n",
    "#%% create BoW\n",
    "# Documentation:\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
    "vectorizer = CountVectorizer(analyzer = 'word',\n",
    "                             tokenizer = None,\n",
    "                             preprocessor = None,\n",
    "                             stop_words = stops,\n",
    "                             max_features = 5000)\n",
    "\n",
    "# fit the vectorizer and return transformed reviews\n",
    "vectorizer.fit(clean_train_reviews)\n",
    "train_data_features = vectorizer.transform(clean_train_reviews)\n",
    "clean_train_reviews=None \n",
    "# convert to numpy array\n",
    "train_data_features = train_data_features.toarray()\n",
    "\n",
    "# create BoW representation of test data\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "clean_test_reviews=None\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Write a PYTHON function `logistic_gradient` that expects a training set matrix `X_train`, a ground truth label vector `y_train` and a current weight vector `w` as its input and returns the gradient `g` of the negative log-likelihood function of the logistic regression. Refer to the lecture notes for the mathematical definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    # https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "    z = np.exp(-np.abs(x))\n",
    "    return np.where(x>=0.0,1.0/(1.0+z),z/(1.0+z))\n",
    "\n",
    "def logistic_gradient(X_train,y_train,w,reg=0.0):\n",
    "    # Rows are variables, Columns are samples\n",
    "    g=np.dot(X_train*np.expand_dims(y_train,0),-sigmoid(-np.dot(X_train.T, w)*y_train))/X_train.shape[1]+2*reg*w\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Write a PYTHON function `find_w` that expects a training set matrix `X_train`, a ground truth label vector `y_train`, a step size `alpha` and a maximum iteration number `max_it` that determines the optimal logistic regression weight vector `w_star` by performing gradient descent, i.e. calling `logistic_gradient` in each iteration. Make sure to include the affine offset $w_0=b$ into your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_w(X_train, y_train, alpha, n_epochs, n_minibatch, reg=0.0,w_init=None):\n",
    "    \n",
    "    X_offs=np.ones((X_train.shape[0]+1,X_train.shape[1]))\n",
    "    X_offs[1:,:]=X_train\n",
    "    w=np.ones((X_offs.shape[0]))\n",
    "    if w_init is not None:\n",
    "        w=w_init\n",
    "    loss=np.inf\n",
    "    for epoch in range(n_epochs):\n",
    "        zzz=-np.log(sigmoid(np.expand_dims(y_train,0)*np.dot(w.T,X_offs)))\n",
    "        loss=np.sum(zzz)/X_offs.shape[1]+reg*np.sum(w**2)\n",
    "        print('Epoch:', epoch+1, ' Current loss:', loss)\n",
    "        rp=np.random.permutation(X_offs.shape[1])\n",
    "        for it in range(X_offs.shape[1]//n_minibatch):\n",
    "            delta_w=logistic_gradient(X_offs[:,rp[it*n_minibatch:(it+1)*n_minibatch]],\n",
    "                                      y_train[rp[it*n_minibatch:(it+1)*n_minibatch]],w,reg)*alpha\n",
    "            w-=np.array(delta_w)\n",
    "        w_star=w\n",
    "    return w_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) The dataset at hand is quite large. Applying standard gradient descent could likely lead to Python throwing a `MemoryError` exception. To avoid this, we will employ a variation of *stochastic gradient descent* that has been proven successful in training deep neural networks. In *minibatch learning*, each iteration of the algorithm is replaced by a so-called *epoch*. In each epoch the training set is divided randomly into subsets of equal size, the minibatches. For each minibatch, the gradient is computed and applied only with respect to the samples in the minibatch. An epoch is finished when the gradient step was performed for each minibatch. Modify `find_w` such that it is capable of mini-batch learning. You will need to replace `max_it` by `n_epochs` and and add the parameter `n_minibatch` to your function definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "d) Write a function `classify_log` that expects a weight vector `w` and a test set matrix `X_test` and classifies the samples in `X_test` via logistic regression, returning a label vector `y_test`. Test your implementation of `find_w` and`classify_log` on `train_data_features` and `test_data_features` with 10 epochs, minibatches of size 100 and step size `alpha=1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "\n",
    "\n",
    "def classify_log(w, X_test):\n",
    "    w0=w[0]\n",
    "    y_test=sigmoid(w0+np.dot(X_test.T,w[1:]))\n",
    "    return y_test\n",
    "\n",
    "#Testing implementation\n",
    "y_train=train['sentiment'].values\n",
    "y_test=test['sentiment'].values\n",
    "w=find_w(train_data_features.T,(y_train-0.5)*2,1,20,100)\n",
    "y_pred=classify_log(w,test_data_features.T)\n",
    "auc = AUC( y_test, y_pred )\n",
    "print('AUC score after 20 epochs:',auc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Logistic regression is prone to *overfitting*. To prevent this, *regularizers* can be used. Adjust your implementation in such a way that instead of minimizing $F(\\mathbf{w})$, it minimizes the term\n",
    "    \t\\begin{equation}\n",
    "    \tF(\\mathbf{w})+\\lambda\\|\\mathbf{w}\\|^2,\n",
    "    \t\\end{equation}\n",
    "    \twhere $\\lambda$ is a non-negative regularization parameter. Test your implementation with $\\lambda=10^{-3}$.  Did the results improve? Why/why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def find_w(X_train, y_train, alpha, n_epochs, n_minibatch, reg=0.0,w_init=None):\n",
    "\n",
    "w=find_w(train_data_features.T,(y_train-0.5)*2,1,20,100,1e-3)\n",
    "y_pred=classify_log(w,test_data_features.T)\n",
    "auc = AUC( y_test, y_pred )\n",
    "print('AUC score after 20 epochs:',auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
